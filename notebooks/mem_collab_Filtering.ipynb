{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "350193e3",
   "metadata": {},
   "source": [
    "# <u> Memory-based Collaborative Filtering</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d30aaa",
   "metadata": {},
   "source": [
    "This notebook explores **memory-based collaborative filtering** as a first baseline for building a movie recommendation system. The approach relies solely on the raw user–item rating matrix, without the need for training or any machine learning framework.\n",
    "\n",
    "The general idea is to build recommendations based on similarities, which can be computed in two main ways:\n",
    "\n",
    "- **User–User Collaborative Filtering**: recommends movies liked by users who have similar preferences and rating behavior to us.\n",
    "- **Item–Item Collaborative Filtering**: recommends new movies that are similar to the ones we have already seen and liked.\n",
    "\n",
    "<br>\n",
    "\n",
    "To define similarities between users or items, we commonly use two metrics:\n",
    "\n",
    "- **Cosine Similarity**: a fast and popular measure that computes the angle between two vectors (single users or items). The formula is:\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\text{sim}_{\\text{cosine}}(x, y) = \\frac{x \\cdot y}{\\|x\\| \\|y\\|}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "- **Pearson Correlation**: more computationally demanding, but it measures the linear relationship between co-rated items, correcting for each user’s individual rating bias. The formula is:\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\text{sim}_{\\text{pearson}}(x, y) = \\frac{\\sum_{i}(x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum(x_i - \\bar{x})^2} \\sqrt{\\sum(y_i - \\bar{y})^2}}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "Due to its ability to correct for individual rating biases, Pearson correlation is especially useful in user–user collaborative filtering, where users may have very different rating scales. In item–item collaborative filtering, this adjustment is less critical, and both Pearson and cosine similarity often produce similar results, particularly in sparse datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c809320",
   "metadata": {},
   "source": [
    "## <u>0. Setting:</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399cabae",
   "metadata": {},
   "source": [
    "### <u>0.1 Import libraries and dataframe</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "df2c32bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd, numpy as np, os, sys, seaborn as sns, matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy.stats import pearsonr\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Set the working directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(current_dir, \"..\"))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# Import module for data processing\n",
    "from modules.data_analysis import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5d6e11b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>movie_bayes_avg</th>\n",
       "      <th>log_count_review</th>\n",
       "      <th>release_year</th>\n",
       "      <th>user_avg_rating</th>\n",
       "      <th>user_avg_bayes</th>\n",
       "      <th>title</th>\n",
       "      <th>...</th>\n",
       "      <th>Film-Noir</th>\n",
       "      <th>Horror</th>\n",
       "      <th>IMAX</th>\n",
       "      <th>Musical</th>\n",
       "      <th>Mystery</th>\n",
       "      <th>Romance</th>\n",
       "      <th>Sci-Fi</th>\n",
       "      <th>Thriller</th>\n",
       "      <th>War</th>\n",
       "      <th>Western</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2005-04-02 23:53:47</td>\n",
       "      <td>3.212004</td>\n",
       "      <td>10.009828</td>\n",
       "      <td>1995</td>\n",
       "      <td>3.742857</td>\n",
       "      <td>3.630537</td>\n",
       "      <td>Jumanji (1995)</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2005-04-02 23:31:16</td>\n",
       "      <td>3.950567</td>\n",
       "      <td>9.050289</td>\n",
       "      <td>1995</td>\n",
       "      <td>3.742857</td>\n",
       "      <td>3.630537</td>\n",
       "      <td>City of Lost Children, The (Cité des enfants p...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2005-04-02 23:33:39</td>\n",
       "      <td>3.897763</td>\n",
       "      <td>10.713995</td>\n",
       "      <td>1995</td>\n",
       "      <td>3.742857</td>\n",
       "      <td>3.630537</td>\n",
       "      <td>Twelve Monkeys (a.k.a. 12 Monkeys) (1995)</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating           timestamp  movie_bayes_avg  \\\n",
       "0       1        2     3.5 2005-04-02 23:53:47         3.212004   \n",
       "1       1       29     3.5 2005-04-02 23:31:16         3.950567   \n",
       "2       1       32     3.5 2005-04-02 23:33:39         3.897763   \n",
       "\n",
       "   log_count_review  release_year  user_avg_rating  user_avg_bayes  \\\n",
       "0         10.009828          1995         3.742857        3.630537   \n",
       "1          9.050289          1995         3.742857        3.630537   \n",
       "2         10.713995          1995         3.742857        3.630537   \n",
       "\n",
       "                                               title  ... Film-Noir  Horror  \\\n",
       "0                                     Jumanji (1995)  ...         0       0   \n",
       "1  City of Lost Children, The (Cité des enfants p...  ...         0       0   \n",
       "2          Twelve Monkeys (a.k.a. 12 Monkeys) (1995)  ...         0       0   \n",
       "\n",
       "   IMAX  Musical  Mystery  Romance  Sci-Fi  Thriller  War  Western  \n",
       "0     0        0        0        0       0         0    0        0  \n",
       "1     0        0        1        0       1         0    0        0  \n",
       "2     0        0        1        0       1         1    0        0  \n",
       "\n",
       "[3 rows x 30 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import cleand dataframe\n",
    "file_path = '../data/processed/ratings_enriched.parquet'\n",
    "rating_enriched = pd.read_parquet(file_path, engine=\"pyarrow\")\n",
    "rating_enriched.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6df43ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique userId: 138383\n",
      "Number of unique movieId: 12531\n",
      "Number of reviews: ~ 19.9 M\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of unique userId: {rating_enriched['userId'].nunique()}\")\n",
    "print(f\"Number of unique movieId: {rating_enriched['movieId'].nunique()}\")\n",
    "print(f\"Number of reviews: ~ {len(rating_enriched)/1000000:.1f} M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d000ea",
   "metadata": {},
   "source": [
    "### <u>0.2 Model evaluation </U>\n",
    "In order to evaluate model performance and enable fair comparison with future approaches, we adopt a Leave-5-out strategy: for each user, the last 5 ratings are held out for testing, while the rest are used for training. This mirrors real-world scenarios where unseen items are recommended based on past behavior.\n",
    "We assess accuracy using Root Mean Squared Error (RMSE) between predicted and actual ratings on the held-out items. This evaluation setup provides a consistent and realistic benchmark across all models.\n",
    "\n",
    "The choice of holding out 5 ratings per user ensures a good balance between training size and evaluation coverage. Since we only include users with at least 20 ratings, this results in a minimum 75% train and 25% test split per user. Based on the user activity distribution observed in the eda.fe.ipynb notebook, this threshold offers sufficient signal for training while ensuring each user contributes to model evaluation.\n",
    "\n",
    "Additionally with approximately 138,000 unique users in the final dataset, this strategy yields around 690,000 test points for evaluation, out of nearly 20 million total ratings, providing robust and reliable validation across the user base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4cf1733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort in term of review date and user id and then pick 10 most recent review for each userId\n",
    "sorted_df = rating_enriched.sort_values(by=['userId','timestamp'], ascending=[True,True])\n",
    "test_df = sorted_df.groupby('userId').tail(5)\n",
    "\n",
    "# Build train df by removing the test_df rows from it\n",
    "train_df = rating_enriched.drop(test_df.index).reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9309d182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assert correct length across users in the test set\n",
    "assert test_df.groupby('userId').size().eq(5).all(), \"Some users in test_df have ≠ 5 ratings\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c38ac292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual Check of test_train split\n",
    "user_id = 1\n",
    "\n",
    "# All ratings for that user, sorted as in the previous setting\n",
    "user_ratings = rating_enriched[rating_enriched['userId'] == user_id].sort_values('timestamp', ascending=True)\n",
    "\n",
    "# Check that the 5 most recent ratings match test_df's entries for that user\n",
    "expected_test_rows = user_ratings.tail(5).reset_index(drop=True)\n",
    "actual_test_rows = test_df[test_df['userId'] == user_id].reset_index(drop=True)\n",
    "\n",
    "# Assert that they match\n",
    "assert expected_test_rows[['movieId', 'timestamp']].equals(actual_test_rows[['movieId', 'timestamp']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e468e7e7",
   "metadata": {},
   "source": [
    "## <u> 1. User-User Collaborative Filtering </u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f9691b",
   "metadata": {},
   "source": [
    "As previously explained, the main idea behind the **user–user collaborative filtering** approach is to recommend items to a target user by finding other users with similar tastes, then suggesting items they liked but the target user hasn't seen yet.\n",
    "\n",
    "To compute similarities across users, we use Pearson **correlation** on the **user–item rating matrix**, which is structured as follows:\n",
    "- Rows represent users (`userId`)\n",
    "- Columns represent movies (`movieId`)\n",
    "- Values are the ratings users assigned to each movie\n",
    "\n",
    "This results in a very sparse matrix, as the dataset contains approximately 139,000 users and 12,000 movies, and most users rate only a small fraction of all available movies.\n",
    "\n",
    "The prediction for how much a user $u$ will like an unseen item $i$ is computed using a **similarity-weighted average** of the ratings from the most similar users who have rated item $i$:\n",
    "\n",
    "$$\n",
    "\\hat{r}_{u,i} = \\bar{r}_u + \\frac{\\sum_{v \\in N(u)} \\text{sim}(u,v) \\cdot (r_{v,i} - \\bar{r}_v)}{\\sum_{v \\in N(u)} |\\text{sim}(u,v)|}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\hat{r}_{u,i}$ is the predicted rating for user $u$ on item $i$\n",
    "- $\\bar{r}_u$ is the average rating of user $u$\n",
    "- $N(u)$ is the set of top-$K$ most similar users to $u$ who rated item $i$\n",
    "- $\\text{sim}(u,v)$ is the Pearson correlation between users $u$ and $v$\n",
    "- $r_{v,i}$ is the rating that user $v$ gave to item $i$\n",
    "- $\\bar{r}_v$ is the average rating of user $v$\n",
    "\n",
    "<br>\n",
    "\n",
    "For the sake of efficiency and prediction stability, we set $K=20$, meaning that predictions are based on the 20 most similar users who have rated the item. This value offers a good trade-off: it is large enough to smooth out noise from individual ratings, while still focusing on the most relevant users.\n",
    "Additionally, by setting the minimum number of reviews per movie to 25, we ensure that each item has enough rating data to support consistent top-K neighbor selection during prediction.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f9536a",
   "metadata": {},
   "source": [
    "### <u>Preparation</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e287f51",
   "metadata": {},
   "source": [
    "#### <u> 1.1.1 Build user-item rating matrix:</u> \n",
    "\n",
    "Given the large size of the user–item matrix, we use `csr_matrix` to store it efficiently in a sparse format.\n",
    "This allows us to keep only the non-zero ratings and their positions, instead of allocating memory for the full (mostly empty) matrix. Since the sparse matrix stores only index-based positions, we keep the original userId and movieId mappings using LabelEncoder to translate between matrix indices and real IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3b23b220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out unnecessary features\n",
    "df_u_u = train_df[['userId','movieId','rating', 'title']].copy()\n",
    "\n",
    "# Encode userId and movieId to ensure compatible indexing with csr_matrix\n",
    "user_encoder = LabelEncoder()\n",
    "movie_encoder = LabelEncoder()\n",
    "df_u_u['user_idx'] = user_encoder.fit_transform(df_u_u['userId'])\n",
    "df_u_u['item_idx'] = movie_encoder.fit_transform(df_u_u['movieId'])\n",
    "\n",
    "# Build spare matrix with efficient allocation of memory\n",
    "user_item_sparse = csr_matrix(\n",
    "    (df_u_u['rating'], (df_u_u['user_idx'], df_u_u['item_idx']))\n",
    ")\n",
    "\n",
    "# Compute transpose for easier acess during computation\n",
    "item_user_sparse = user_item_sparse.T.tocsr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8ca3bd",
   "metadata": {},
   "source": [
    "#### <u> 1.1.2 Compute users rating mean:</u> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fd60bca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute mean of each users ($\\hat{r}_u$)\n",
    "user_means = df_u_u.groupby('userId')['rating'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b06be25",
   "metadata": {},
   "source": [
    "#### <u> 1.1.3 Pearson computation function:</u> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e3d4143d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pearson(u_vec, v_vec):\n",
    "\n",
    "    '''\n",
    "    Compute pearson similarity across two users based on shared movies.\n",
    "    '''\n",
    "\n",
    "    # Find intersection of movies watched across user u and v\n",
    "    common = np.intersect1d(u_vec.indices, v_vec.indices)\n",
    "\n",
    "    # If not enough overalapping return 0\n",
    "    if len(common) < 2:\n",
    "        return 0\n",
    "    \n",
    "    # Compute similaries based on shared movies\n",
    "    u_ratings = u_vec[:, common].toarray().flatten()\n",
    "    v_ratings = v_vec[:, common].toarray().flatten()\n",
    "\n",
    "\n",
    "     # Return 0 if either user has constant ratings\n",
    "    if np.std(u_ratings) == 0 or np.std(v_ratings) == 0:\n",
    "        return 0\n",
    "\n",
    "    # Compute Pearson similarity\n",
    "    sim, _ = pearsonr(u_ratings, v_ratings)\n",
    "    return sim if not np.isnan(sim) else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbffca10",
   "metadata": {},
   "source": [
    "#### <u>1.1.4 Prediction function on test set:</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1ece7c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rating(u_idx, i_idx, K=20):\n",
    "\n",
    "    '''\n",
    "    Predict the rating of user u_idx on item i_idx based on the K most similar users.\n",
    "    '''\n",
    "    \n",
    "    # Get sparse rating vector of target user u\n",
    "    u_vec = user_item_sparse[u_idx]\n",
    "\n",
    "    # Get all users who rated item i\n",
    "    users_who_rated_i = item_user_sparse[i_idx].indices\n",
    "\n",
    "    similarities = []\n",
    "    ratings = []\n",
    "\n",
    "    for v_idx in users_who_rated_i:\n",
    "        \n",
    "        # Skip the target user\n",
    "        if v_idx == u_idx:\n",
    "            continue\n",
    "        \n",
    "        # Compute Pearson correlation between user u and neighbor v\n",
    "        sim = compute_pearson(u_vec, user_item_sparse[v_idx])\n",
    "        if sim <= 0:\n",
    "            continue\n",
    "\n",
    "        # Get user v's rating on item i\n",
    "        r_vi = user_item_sparse[v_idx, i_idx]\n",
    "\n",
    "        # Get user v's average rating\n",
    "        r_v_mean = user_means.get(v_idx, 0)\n",
    "\n",
    "        # Store similarity and the rating deviation\n",
    "        similarities.append(sim)\n",
    "        ratings.append((r_vi - r_v_mean, sim))\n",
    "\n",
    "    # If no valid neighbors, fall back to user mean or 3.0\n",
    "    if not ratings:\n",
    "        return user_means.get(u_idx, 3.0)\n",
    "\n",
    "    # Sort neighbors by absolute similarity and select top K\n",
    "    ratings = sorted(ratings, key=lambda x: x[1], reverse=True)[:K]\n",
    "\n",
    "    # Compute weighted average of rating deviations\n",
    "    numerator = sum((r * s) for r, s in ratings)\n",
    "    denominator = sum(abs(s) for _, s in ratings)\n",
    "\n",
    "    # Avoid division by zero (in case similarities cancel out)\n",
    "    if denominator == 0:\n",
    "        return user_means.get(u_idx, 3.0)\n",
    "\n",
    "    # Return user mean + similarity-weighted adjustment\n",
    "    r_u_mean = user_means.get(u_idx, 3.0)\n",
    "    return r_u_mean + numerator / denominator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bb3904",
   "metadata": {},
   "source": [
    "### <u>1.2 Evaluation on test set </u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "08d7940c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "911e89254c7a40948d64d8b37a4b211c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/691915 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m true \u001b[38;5;241m=\u001b[39m row\u001b[38;5;241m.\u001b[39mrating\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Predict rating\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_rating\u001b[49m\u001b[43m(\u001b[49m\u001b[43mu_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Append results\u001b[39;00m\n\u001b[0;32m     19\u001b[0m pred_ratings\u001b[38;5;241m.\u001b[39mappend(pred)\n",
      "Cell \u001b[1;32mIn[27], line 23\u001b[0m, in \u001b[0;36mpredict_rating\u001b[1;34m(u_idx, i_idx, K)\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Compute Pearson correlation between user u and neighbor v\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m sim \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_pearson\u001b[49m\u001b[43m(\u001b[49m\u001b[43mu_vec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_item_sparse\u001b[49m\u001b[43m[\u001b[49m\u001b[43mv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sim \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[26], line 8\u001b[0m, in \u001b[0;36mcompute_pearson\u001b[1;34m(u_vec, v_vec)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03mCompute pearson similarity across two users based on shared movies.\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Find intersection of movies watched across user u and v\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m common \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintersect1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mu_vec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv_vec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# If not enough overalapping return 0\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(common) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\numpy\\lib\\arraysetops.py:455\u001b[0m, in \u001b[0;36mintersect1d\u001b[1;34m(ar1, ar2, assume_unique, return_indices)\u001b[0m\n\u001b[0;32m    453\u001b[0m     aux \u001b[38;5;241m=\u001b[39m aux[aux_sort_indices]\n\u001b[0;32m    454\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 455\u001b[0m     \u001b[43maux\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    457\u001b[0m mask \u001b[38;5;241m=\u001b[39m aux[\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;241m==\u001b[39m aux[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    458\u001b[0m int1d \u001b[38;5;241m=\u001b[39m aux[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][mask]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Encode test userId and movieId using the same encoders used on the train set\n",
    "test_df['user_idx'] = user_encoder.transform(test_df['userId'])\n",
    "test_df['item_idx'] = movie_encoder.transform(test_df['movieId'])\n",
    "\n",
    "# Initialize prediction and ground truth lists\n",
    "pred_ratings = []\n",
    "true_ratings = []\n",
    "\n",
    "# Loop through each row of the test set\n",
    "for row in tqdm(test_df.itertuples(index=False), total=len(test_df)):\n",
    "    u_idx = row.user_idx\n",
    "    i_idx = row.item_idx\n",
    "    true = row.rating\n",
    "\n",
    "    # Predict rating\n",
    "    pred = predict_rating(u_idx, i_idx, K=20)\n",
    "\n",
    "    # Append results\n",
    "    pred_ratings.append(pred)\n",
    "    true_ratings.append(true)\n",
    "\n",
    "# Compute RMSE\n",
    "rmse = mean_squared_error(true_ratings, pred_ratings, squared= False)\n",
    "print(f\"Test RMSE: {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3691b90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
